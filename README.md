Avance 1 - 15% (Definici√≥n de alternativa - Diagramas de clases - Soluci√≥n preliminar)

# üåê Sistema WebScrapping

## üóíÔ∏è Colaboradores

```
‚îú‚îÄ‚îÄ Santiago Gamboa Mart√≠nez
‚îú‚îÄ‚îÄ Samuel Eduardo Fajardo Quintero
‚îî‚îÄ‚îÄ Manuel Felipe Torres Gamboa
```

## üèÜ Introducci√≥n

En el contexto actual, donde la cantidad de informaci√≥n disponible en internet crece de manera exponencial, la extracci√≥n eficiente de datos relevantes se ha convertido en una necesidad crucial para diversos sectores. El desarrollo de un sistema de web scraping representa una soluci√≥n tecnol√≥gica para obtener, procesar y analizar informaci√≥n de forma automatizada desde sitios web. Por esta raz√≥n, como equipo, hemos elegido desarrollar la `alternativa 2`: **_Sistema de WebScrapping_**, este proyecto propone el dise√±o e implementaci√≥n de un sistema de web scraping que no solo cumpla con los objetivos de extracci√≥n de datos, sino que est√© estructurado bajo los principios fundamentales de la Programaci√≥n Orientada a Objetos (POO).

La elecci√≥n de POO como paradigma principal radica en su capacidad para ofrecer una arquitectura modular, reutilizable y escalable, cualidades esenciales en un proyecto de esta naturaleza. Mediante el uso de conceptos como encapsulaci√≥n, herencia, composici√≥n, polimorfismo y abstracci√≥n, el sistema garantizar√° un dise√±o robusto que permita extender sus funcionalidades de manera eficiente y manejar la complejidad inherente al tratamiento de datos en entornos est√°ticos y din√°micos.

Este proyecto no solo busca cumplir con los objetivos t√©cnicos del desarrollo de un sistema funcional, sino tambi√©n destacar el valor pedag√≥gico de aplicar los pilares de POO en un contexto pr√°ctico. De esta manera, se contribuye al fortalecimiento de habilidades de dise√±o y programaci√≥n, aline√°ndose con los requerimientos acad√©micos y profesionales.

## ‚ûï Definici√≥n de Alternativa

La alternativa para este proyecto consiste en el desarrollo de un sistema de web scraping que emplee como pilar principal la Programaci√≥n Orientada a Objetos (POO). El sistema ser√° desarrollado en Python, un lenguaje ampliamente reconocido por su versatilidad y su extenso ecosistema de librer√≠as dise√±adas para la extracci√≥n y manipulaci√≥n de datos desde la web.

### Caracter√≠sticas principales:

1. **Aplicaci√≥n de POO**:  
   El dise√±o del sistema estar√° basado en los pilares de POO:

   - **Encapsulaci√≥n** para proteger y organizar datos.
   - **Abstracci√≥n** para simplificar la interacci√≥n con el sistema.
   - **Herencia** para evitar redundancias y fomentar la reutilizaci√≥n de c√≥digo.
   - **Polimorfismo** para permitir extensibilidad y personalizaci√≥n.

2. **Uso de Python y sus librer√≠as especializadas**:

   - **Requests**: Para realizar solicitudes HTTP y acceder al contenido de las p√°ginas web.
   - **BeautifulSoup** (de la biblioteca bs4): Para parsear HTML y extraer informaci√≥n estructurada.
   - **Selenium** (si se requiere interacci√≥n din√°mica): Para manejar sitios web que cargan contenido de forma as√≠ncrona o que requieren simulaci√≥n de usuarios.
   - **Pandas**: Para procesar y almacenar los datos extra√≠dos en formatos como CSV o bases de datos.

3. **Requerimientos tecnol√≥gicos adicionales**:
   - **Gesti√≥n de dependencias**: Uso de herramientas como `pip` o archivos `requirements.txt` para garantizar la instalaci√≥n de librer√≠as necesarias.
   - **Control de versiones**: Implementaci√≥n de Git para mantener un historial claro del desarrollo y facilitar la colaboraci√≥n en equipo.
   - **Entorno de ejecuci√≥n**: Uso de entornos virtuales (`venv`) para asegurar compatibilidad y aislamiento de dependencias.
   - **Persistencia de datos**: Almacenamiento de la informaci√≥n extra√≠da en formatos accesibles, como bases de datos SQLite o archivos JSON/CSV.

### Ventajas de esta alternativa:

- Facilita la organizaci√≥n y escalabilidad del sistema gracias a la implementaci√≥n de POO.
- Aprovecha el amplio ecosistema de Python, que incluye librer√≠as maduras y bien documentadas.
- Brinda flexibilidad para adaptarse a diversas necesidades, como la extracci√≥n de datos est√°ticos o din√°micos.
- Fomenta la adquisici√≥n de habilidades de dise√±o y codificaci√≥n estructurada para su aplicaci√≥n en escenarios reales.

## üìà Diagrama de Clases

```mermaid
classDiagram
    %% Clase base: Scraper
    class Scraper {
        - base_url: str
        - headers: dict
        + fetch_data(): abstract
        + handle_errors(error): void
    }

    %% Clase derivada: StaticScraper
    class StaticScraper {
        + fetch_data(): BeautifulSoup
        + parse_data(soup, selector): list
    }

    %% Clase derivada: DynamicScraper
    class DynamicScraper {
        - driver_path: str
        + fetch_data(): BeautifulSoup
        + parse_data(soup, selector): list
    }

    %% Clase para gesti√≥n de datos: DataManager
    class DataManager {
        - data: list
        + add_data(extracted_data): void
        + save_to_csv(file_name): void
    }

    %% Clase controladora: WebScrapingApp
    class WebScrapingApp {
        - scraper: Scraper
        - data_manager: DataManager
        + start_scraping(selector): void
    }

    %% Relaciones
    Scraper <|-- StaticScraper : herencia
    Scraper <|-- DynamicScraper : herencia
    WebScrapingApp o-- Scraper : composici√≥n
    WebScrapingApp o-- DataManager : composici√≥n
```

### **Explicaci√≥n de los Pilares de POO**

1. **Herencia**  
   - **D√≥nde se aplica**: La clase `Scraper` es la clase base abstracta de la cual derivan `StaticScraper` y `DynamicScraper`.  
   - **Por qu√© es importante**: Permite reutilizar c√≥digo com√∫n entre los diferentes tipos de scrapers, reduciendo duplicaci√≥n y facilitando mantenimiento.  

2. **Abstracci√≥n**  
   - **D√≥nde se aplica**: La clase `Scraper` define el m√©todo abstracto `fetch_data`, que las subclases implementan seg√∫n sus propias necesidades.  
   - **Por qu√© es importante**: Oculta los detalles espec√≠ficos del scraping (est√°tico o din√°mico) al usuario del sistema, ofreciendo una interfaz clara.  

3. **Polimorfismo**  
   - **D√≥nde se aplica**: El m√©todo `fetch_data` es implementado de manera distinta en `StaticScraper` y `DynamicScraper`, pero ambas subclases pueden ser usadas de manera intercambiable en el sistema.  
   - **Por qu√© es importante**: Permite que el sistema maneje diferentes tipos de scrapers sin necesidad de modificar el c√≥digo principal de la aplicaci√≥n.  

4. **Encapsulaci√≥n**  
   - **D√≥nde se aplica**: Atributos como `base_url`, `headers` y `driver_path` son protegidos o privados, asegurando que solo las propias clases o subclases tengan acceso directo a ellos.  
   - **Por qu√© es importante**: Mantiene los datos seguros y previene que sean modificados de forma indebida desde fuera de las clases.  

5. **Composici√≥n**  
   - **D√≥nde se aplica**: La clase `WebScrapingApp` utiliza instancias de `Scraper` (o sus subclases) y `DataManager`.  
   - **Por qu√© es importante**: Fomenta la modularidad, ya que permite que los componentes del sistema interact√∫en entre s√≠ sin estar fuertemente acoplados.  

## üíø Soluci√≥n Preliminar

#### **Objetivo General**

Desarrollar un sistema de web scraping basado en la Programaci√≥n Orientada a Objetos (POO) que permita extraer, procesar y almacenar informaci√≥n de manera eficiente, utilizando Python y sus herramientas tecnol√≥gicas especializadas.

---

#### **Arquitectura del Proyecto**

El proyecto estar√° compuesto por los siguientes m√≥dulos principales, dise√±ados bajo los principios de POO:

1. **Clase Base: `Scraper`**

   - **Responsabilidad**:
     - Actuar como la clase abstracta base para otros scrapers espec√≠ficos.
     - Definir m√©todos comunes como establecer conexi√≥n con p√°ginas web y manejo de errores.
   - **Atributos**:
     - `base_url`: URL del sitio objetivo.
     - `headers`: Cabeceras HTTP para simular navegadores.
   - **M√©todos Abstractos**:
     - `fetch_data()`: M√©todo abstracto que las subclases implementar√°n seg√∫n las necesidades de extracci√≥n.

2. **Clase Derivada: `StaticScraper`**

   - **Responsabilidad**:
     - Extraer informaci√≥n de sitios con contenido est√°tico.
   - **Librer√≠as Utilizadas**:
     - `requests`, `BeautifulSoup`.
   - **M√©todos Clave**:
     - `fetch_data()`: Descarga el contenido HTML y lo parsea con BeautifulSoup.
     - `parse_data()`: Extrae informaci√≥n espec√≠fica como tablas, encabezados o enlaces.

3. **Clase Derivada: `DynamicScraper`**

   - **Responsabilidad**:
     - Manejar sitios con contenido din√°mico que requiere interacci√≥n, utilizando Selenium.
   - **Librer√≠as Utilizadas**:
     - `Selenium`, `webdriver`.
   - **M√©todos Clave**:
     - `fetch_data()`: Interact√∫a con el sitio web simulando acciones del usuario.
     - `parse_data()`: Extrae contenido generado din√°micamente.

4. **Clase para Gesti√≥n de Datos: `DataManager`**

   - **Responsabilidad**:
     - Procesar y almacenar los datos extra√≠dos.
   - **Atributos**:
     - `data`: Estructura de datos para mantener la informaci√≥n temporalmente.
   - **M√©todos Clave**:
     - `save_to_csv()`: Guarda los datos en un archivo CSV.
     - `save_to_database()`: Almacena los datos en SQLite o cualquier base de datos relacional.

5. **Clase Controladora: `WebScrapingApp`**
   - **Responsabilidad**:
     - Coordinar la interacci√≥n entre las diferentes clases y gestionar el flujo del programa.
   - **M√©todos Clave**:
     - `start_scraping()`: Inicia el proceso de scraping seg√∫n el tipo de sitio web (est√°tico o din√°mico).
     - `display_results()`: Muestra los datos extra√≠dos en consola o interfaz gr√°fica.

---

#### **Requerimientos T√©cnicos**

1. **Lenguaje y Herramientas**:

   - **Python**: Lenguaje principal.
   - **Librer√≠as**:
     - `requests`, `BeautifulSoup` para scraping est√°tico.
     - `Selenium` para scraping din√°mico.
     - `Pandas` para procesar y almacenar datos.
     - `sqlite3` o `SQLAlchemy` para manejo de bases de datos.

2. **Entorno de Desarrollo**:

   - Uso de entornos virtuales (`venv`) para la instalaci√≥n de dependencias.
   - Control de versiones con Git.

3. **Formato de Almacenamiento**:
   - Datos exportados a archivos CSV o almacenados en bases de datos SQLite para an√°lisis posterior.

---

#### **Plan de Implementaci√≥n**

1. **Fase 1: Dise√±o**

   - Diagramar la arquitectura del sistema utilizando diagramas UML (clases, relaciones).
   - Definir los atributos y m√©todos para cada clase seg√∫n las necesidades del proyecto.

2. **Fase 2: Desarrollo**

   - Implementar la clase base y las derivadas (`Scraper`, `StaticScraper`, `DynamicScraper`).
   - Desarrollar el m√≥dulo de gesti√≥n de datos (`DataManager`).
   - Implementar la clase controladora (`WebScrapingApp`).

3. **Fase 3: Pruebas**

   - Realizar pruebas unitarias para cada clase.
   - Ejecutar pruebas funcionales para validar el flujo completo del sistema.

4. **Fase 4: Documentaci√≥n y Entrega**
   - Documentar el c√≥digo utilizando est√°ndares como docstrings.
   - Crear un manual de usuario y una gu√≠a t√©cnica del proyecto.

---

#### **Resultados Esperados**

- Sistema funcional capaz de extraer datos de sitios web est√°ticos y din√°micos.
- Almacenamiento organizado de los datos en formatos accesibles.
- C√≥digo modular, reutilizable y escalable, que cumpla con los principios de POO.