# üåê Sistema WebScrapping

## üóíÔ∏è Colaboradores

```
‚îú‚îÄ‚îÄ Santiago Gamboa Mart√≠nez
‚îú‚îÄ‚îÄ Samuel Eduardo Fajardo Quintero
‚îî‚îÄ‚îÄ Manuel Felipe Torres Gamboa
```

# üèÜ Introducci√≥n

El volumen de informaci√≥n que se encuentra disponible en internet crece de manera exponencial, haciendo indispensable el uso de herramientas tecnol√≥gicas que permitan extraer y analizar datos relevantes de forma autom√°tica y eficiente. Por esta raz√≥n, como equipo, hemos elegido desarrollar la `alternativa 2`: **_Sistema de WebScrapping_**, este proyecto consiste en desarrollar e implementar un sistema de web scraping que no solo cumpla con los objetivos de extracci√≥n de datos, sino que tambi√©n est√© estructurado bajo los principios fundamentales de la Programaci√≥n Orientada a Objetos (POO).

Este proyecto, titulado: "Super_Proyecto_Final", tiene como objetivo dise√±ar un sistema de web scraping estructurado bajo principios de modularidad y escalabilidad. Para ello, se emplear√°n herramientas como Python junto a librer√≠as especializadas como Requests, BeautifulSoup, Selenium y Pandas. Adem√°s, se garantizar√° un desarrollo robusto mediante buenas pr√°cticas, como el manejo adecuado de excepciones y una organizaci√≥n eficiente del c√≥digo estructurado bajo el paradigma de Programaci√≥n Orientada a Objetos (POO).

# ‚ûï Definici√≥n de Alternativa

La alternativa para este proyecto consiste en el desarrollo de un sistema de web scraping que emplee como pilar principal la Programaci√≥n Orientada a Objetos (POO). El sistema, como ya se mencion√≥ anteriormente, ser√° desarrollado en Python, un lenguaje ampliamente reconocido por su versatilidad y su extenso ecosistema de librer√≠as dise√±adas para la extracci√≥n y manipulaci√≥n de datos desde la web, ademas, se contar√° con la implementaci√≥n de un entorno virtual en el cual se instalar√°n las dependencias necesarias para desarrollar y ejecutar este sistema de web scraping.

### Ventajas de esta alternativa:

- Facilita la organizaci√≥n y escalabilidad del sistema gracias a la implementaci√≥n de Programaci√≥n Orientada a Objetos (POO).
- Aprovecha el amplio ecosistema y la versatilidad de Python, que incluye librer√≠as robustas que a su vez estan bien documentadas.
- Brinda flexibilidad para adaptarse a diversas necesidades, como la extracci√≥n de datos est√°ticos o din√°micos dependiendo el caso.
- Fomenta la adquisici√≥n de habilidades de dise√±o y codificaci√≥n para su aplicaci√≥n en escenarios reales.

# üóÇÔ∏è Requerimientos T√©cnicos:

### 1. **Lenguaje y Librer√≠as**:

- Python como lenguaje principal.

### **Librer√≠as**:

- **`Requests`** para realizar solicitudes HTTP.
- **`BeautifulSoup`** para parsear y extraer datos de HTML.

- **`Selenium`** para interactuar con p√°ginas din√°micas.
- **`Pandas`** para almacenar y procesar datos en estructuras organizadas.

### 2. **Estructura del C√≥digo**:

Implementaci√≥n bajo los principios de la Programaci√≥n Orientada a Objetos (POO) para garantizar modularidad y escalabilidad.

### **Clases principales**:

- **`WebDataExtractor`** (base).
- **`StaticPageExtractor`** y **`DynamicPageExtractor`** (derivadas).
- **`DataHandler`** para gestionar los datos.
- **`ScrapingCoordinator`** para coordinar el flujo del sistema.

### 3. **Entorno de Desarrollo**:

- Uso de entornos virtuales para aislamiento de dependencias (venv).
- Gesti√≥n de versiones con Git para colaboraci√≥n y control del progreso.
- Archivo **`requirements.txt`** para especificar las dependencias del proyecto.

### 4. **Salida de Datos**:

Soporte para formatos CSV, JSON o almacenamiento en bases de datos SQLite.

### 5. **Otros Requerimientos**:

Capacidad de manejar excepciones para evitar interrupciones en la ejecuci√≥n.
Compatibilidad con sitios web tanto est√°ticos como din√°micos.

# üõ†Ô∏è Configuraci√≥n del Entorno de Trabajo

### **1. Clonar el repositorio**:

Descargar el c√≥digo fuente con los siguientes comandos:

```bash
git clone https://github.com/santgm56/Super-Proyecto-Final.git
cd Super-Proyecto-Final
```

### **2. Crear y activar un entorno virtual**:

El uso de un entorno virtual ayuda a instalar las dependencias del proyecto sin interferir con otras aplicaciones de Python.

**En Windows**:

```bash
python -m venv venv
.\venv\Scripts\activate
```

**En macOS/Linux**:

```bash
python -m venv venv
source venv/bin/activate
```

### **3. Instalar las dependencias**:

Una vez dentro del entorno virtual, ejecutar:

```bash
pip install -r requirements.txt
```

### **5. Salir del entorno virtual**:

Al terminar de trabajar o hacer modificaciones, se puede salir del entorno virtual escribiendo:

```bash
deativate
```

### **Nota adicional**:

Si se usa Windows y existe alg√∫n problema al activar el entorno virtual, es posible que se necesite habilitar la ejecuci√≥n de scripts por pol√≠ticas de resticci√≥n en powershell. Para corregirlo, basta con ejecutar estos comandos en el CMD como terminal predeterminada ya que esta no cuenta con dichas condiciones.


# ‚ú® Estructura del proyecto

```plaintext
SUPER_PROYECTO_FINAL/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ web_data_extractor.py     # Clase base WebDataExtractor
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ static_page_extractor.py  # Clase derivada StaticPageExtractor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic_page_extractor.py # Clase derivada DynamicPageExtractor
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_handler.py           # Clase DataHandler
‚îÇ   ‚îú‚îÄ‚îÄ coordinator/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraping_coordinator.py   # Clase ScrapingCoordinator
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ _init_.py
‚îÇ       ‚îî‚îÄ‚îÄ helpers.py                # Funciones auxiliares
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îú‚îÄ‚îÄ test_static_page_extractor.py  # Pruebas unitarias de StaticPageExtractor
‚îÇ   ‚îú‚îÄ‚îÄ test_dynamic_page_extractor.py # Pruebas unitarias de DynamicPageExtractor
‚îÇ   ‚îú‚îÄ‚îÄ test_web_data_extractor.       # Pruebas unitarias de WebDataExtractor
‚îÇ   ‚îú‚îÄ‚îÄ test_data_handler.py           # Pruebas unitarias de DataHandler
‚îÇ   ‚îî‚îÄ‚îÄtest_scraping_coordinator       # Pruebas unitarias de ScrapingCoordinator
‚îú‚îÄ‚îÄ logs/
‚îÇ      error.log                      # Para errores cr√≠ticos.
‚îÇ      activiti.log                   # Para registrar eventos generales del scraping.
‚îî‚îÄ‚îÄ outputs/
```

El proyecto est√° organizado de manera modular y jer√°rquica, siguiendo buenas pr√°cticas de desarrollo de software. A continuaci√≥n, se explica cada componente y su importancia, as√≠ como las ventajas de utilizar esta estructura:

### **1. Archivos Ra√≠z**
- **`README.md`**:  
  - **Importancia**: Es la primera impresi√≥n del proyecto. Proporciona una descripci√≥n general, instrucciones de instalaci√≥n, uso y documentaci√≥n clave.  
  - **Ventajas**: Facilita la comprensi√≥n del proyecto para nuevos desarrolladores o colaboradores. Es esencial para proyectos open-source o colaborativos.  

- **`requirements.txt`**:  
  - **Importancia**: Lista todas las dependencias necesarias para ejecutar el proyecto.  
  - **Ventajas**: Permite replicar el entorno de desarrollo f√°cilmente con `pip install -r requirements.txt`. Asegura que todos los colaboradores usen las mismas versiones de las librer√≠as.  

- **`.gitignore`**:  
  - **Importancia**: Especifica archivos y directorios que no deben ser rastreados por Git (por ejemplo, `__pycache__`, `logs/`, `outputs/`).  
  - **Ventajas**: Evita la inclusi√≥n de archivos innecesarios en el repositorio, como archivos temporales o datos sensibles.  

- **`setup.py`**:  
  - **Importancia**: Script para instalar el proyecto y sus dependencias. Puede incluir metadatos y configuraciones de instalaci√≥n.  
  - **Ventajas**: Facilita la distribuci√≥n e instalaci√≥n del proyecto como un paquete Python.  

- **`main.py`**:  
  - **Importancia**: Punto de entrada principal del scraper. Contiene la l√≥gica para iniciar el proceso de scraping.  
  - **Ventajas**: Centraliza la ejecuci√≥n del proyecto, lo que simplifica la interacci√≥n con el usuario final.  

---

### **2. Directorio `src/` (C√≥digo Fuente)**  
Este directorio contiene el n√∫cleo del proyecto, organizado en m√≥dulos y subdirectorios espec√≠ficos.  

#### **2.1. `config.py`**  
- **Importancia**: Centraliza la configuraci√≥n del proyecto (por ejemplo, timeouts, selectores CSS, credenciales de API).  
- **Ventajas**: Facilita la modificaci√≥n de par√°metros sin necesidad de alterar el c√≥digo fuente. Mejora la mantenibilidad.  

#### **2.2. `base/`**  
- **Importancia**: Contiene la clase base abstracta `web_data_extractor.py`, que define la interfaz com√∫n para todos los extractores.  
- **Ventajas**: Promueve la reutilizaci√≥n de c√≥digo y asegura que todos los extractores sigan un patr√≥n com√∫n (herencia y polimorfismo).  

#### **2.3. `components/`**  
- **Importancia**: Contiene los m√≥dulos espec√≠ficos para el scraping, divididos en:  
  - **`static_page_extractor.py`**: Extracci√≥n de p√°ginas est√°ticas (HTML/CSS).  
  - **`dynamic/`**: Extracci√≥n de p√°ginas din√°micas (JavaScript), con m√≥dulos espec√≠ficos para e-commerce y bienes ra√≠ces.  
  - **`data_handler.py`**: Manejo de datos extra√≠dos (JSON, SQL).  
- **Ventajas**: La modularidad permite agregar nuevos tipos de extractores sin afectar el c√≥digo existente. Facilita las pruebas y el mantenimiento.  

#### **2.4. `coordinator/`**  
- **Importancia**: Contiene `scraping_coordinator.py`, que gestiona el flujo de trabajo del scraping (descarga, extracci√≥n y almacenamiento).  
- **Ventajas**: Centraliza la l√≥gica de coordinaci√≥n, lo que simplifica la ejecuci√≥n de tareas complejas y mejora la escalabilidad.  

#### **2.5. `utils/`**  
- **Importancia**: Proporciona funciones auxiliares, como validaci√≥n de URLs (`helpers.py`) y configuraci√≥n de logging (`logger.py`).  
- **Ventajas**: Promueve la reutilizaci√≥n de c√≥digo y reduce la duplicaci√≥n. Facilita la depuraci√≥n y el monitoreo del proyecto.  

#### **2.6. `db/`**  
- **Importancia**: Contiene los modelos de base de datos (`models.py`) y la configuraci√≥n de la conexi√≥n (`database.py`).  
- **Ventajas**: Separa la l√≥gica de acceso a datos del resto del c√≥digo, lo que facilita la migraci√≥n a otros sistemas de bases de datos.  

#### **2.7. `web/`**  
- **Importancia**: Implementa la API RESTful usando Flask (`app.py`, `routes.py`) y los archivos est√°ticos (`templates/`, `static/`).  
- **Ventajas**: Permite exponer los datos scrapeados a trav√©s de una interfaz web, lo que facilita la integraci√≥n con otros sistemas.  

---

### **3. Directorio `tests/`**  
- **Importancia**: Contiene pruebas automatizadas para cada m√≥dulo del proyecto (`test_modules.py`) y configuraciones comunes (`conftest.py`).  
- **Ventajas**: Asegura la calidad del c√≥digo y detecta errores temprano. Facilita la refactorizaci√≥n y el mantenimiento.  

---

### **4. Directorio `logs/`**  
- **Importancia**: Almacena archivos de registro (`scraping.log`) que documentan la actividad del scraper.  
- **Ventajas**: Facilita la depuraci√≥n y el monitoreo del sistema en producci√≥n.  

---

### **5. Directorio `outputs/`**  
- **Importancia**: Contiene los resultados del scraping en formato JSON o SQL.  
- **Ventajas**: Centraliza los datos extra√≠dos, lo que facilita su an√°lisis y uso posterior.  

---

### **6. Directorio `static/`**  
- **Importancia**: Almacena archivos est√°ticos (CSS, JS, im√°genes) para la interfaz web.  
- **Ventajas**: Separa el contenido est√°tico del c√≥digo din√°mico, lo que mejora el rendimiento y la organizaci√≥n.  

---

### **Ventajas Generales de la Estructura**
1. **Modularidad**: Cada componente tiene una responsabilidad clara, lo que facilita la escalabilidad y el mantenimiento.  
2. **Reutilizaci√≥n de C√≥digo**: Funciones comunes (por ejemplo, logging, manejo de datos) est√°n centralizadas en m√≥dulos reutilizables.  
3. **Claridad y Organizaci√≥n**: La estructura jer√°rquica y los nombres descriptivos hacen que el proyecto sea f√°cil de entender y navegar.  
4. **Escalabilidad**: Nuevos m√≥dulos (por ejemplo, extractores para otros sitios) pueden agregarse sin afectar el c√≥digo existente.  
5. **Colaboraci√≥n**: Facilita el trabajo en equipo al separar responsabilidades y proporcionar una estructura clara.  
6. **Pruebas y Depuraci√≥n**: Las pruebas automatizadas y los logs mejoran la calidad del c√≥digo y simplifican la detecci√≥n de errores.  


# üìà Diagrama de Clases

```mermaid
classDiagram
    class WebDataExtractor {
        <<Abstract>>
        - _url: str
        - _html_content: str
        - _data: List[Dict]
        + logger: Logger
        + download()*: str
        + parse()*: List[Dict]
        + store()*: bool
        + scrape(): List[Dict]
        + iter_data(): Generator
        + url: str
        + html_content: str
        + data: List[Dict]
    }

    class StaticPageExtractor {
        - _selectores: Dict
        + get_selectores(): Dict
        + get_cache_filename(): str
        + download(): str
        + parse(): List[Dict]
        + store(): bool
        + selectores: Dict
    }

    class DynamicPageExtractor {
        - _tienda: str
        - _num_productos: int
        + driver: WebDriver
        + detectar_tienda(): str
        + configurar_driver(): WebDriver
        + download(): str
        + parse()*: List[Dict]
        + store(): bool
        + tienda: str
        + num_productos: int
    }

    class EcommerceExtractor {
        + tienda: str
        + parse(): List[Dict]
        + store(): bool
        + extraer_texto(): str
        + extraer_imagen(): str
        + extraer_precio(): str
        + procesar_descuento(): str
        + extraer_puntuacion(): Dict
        + extraer_url(): str
        + extraer_descripcion(): str
    }

    class RealEstateExtractor {
        + tienda: str
        + ubicacion: str
        + user_agent: str
        + extraer_ubicacion_url(): str
        + configurar_driver(): WebDriver
        + download(): str
        + parse(): List[Dict]
        + store(): bool
        + manejar_popups(): None
        + configurar_tipo_negocio(): None
        + configurar_ubicacion_exacta(): None
        + ejecutar_busqueda(): None
        + extraer_titulo(): str
        + extraer_precio(): str
        + formatear_precio(): str
        + extraer_cardblock(): str
        + extraer_url(): str
    }

    class DataHandler {
        - _data: Union[Dict, List[Dict]]
        - _storage_format: str
        - _logger: Logger
        + store_data(): bool
        + store_json(): bool
        + store_sql(): bool
        + generate_report(): str
        + categorize_data(): Dict
        + data: Union[Dict, List[Dict]]
        + storage_format: str
        + logger: Logger
    }

    class ScrapingCoordinator {
        + tasks: List[Dict]
        + max_workers: int
        + results: List[Dict]
        + validate_tasks(): None
        + select_extractor(): WebDataExtractor
        + process_task(): Dict
        + run(): Dict
    }

    class ScrapedData {
        + id: int
        + url: str
        + tipo: str
        + contenido: str
        + fecha_extraccion: DateTime
    }

    class App {
        + app: Flask
        + create_app(): Flask
        + index(): str
        + not_found(): Dict
        + server_error(): Dict
    }

    class ProductData {
        + title: str
        + image: str
        + price_original: str
        + price_sell: str
        + discount: str
        + rating: Dict
        + url: str
        + description: str
        + to_dict(): Dict
    }

    class Helpers {
        + validate_url(): bool
        + create_directory_structure(): None
        + clean_filename(): str
        + calculate_stats(): Dict
        + generate_hash(): str
    }

    class Logger {
        + setup_logger(): Logger
        + get_logger(): Logger
    }

    WebDataExtractor <|-- StaticPageExtractor
    WebDataExtractor <|-- DynamicPageExtractor
    DynamicPageExtractor <|-- EcommerceExtractor
    DynamicPageExtractor <|-- RealEstateExtractor

    EcommerceExtractor --> ProductData
    EcommerceExtractor --> DataHandler
    RealEstateExtractor --> DataHandler
    ScrapingCoordinator --> WebDataExtractor
    ScrapingCoordinator --> StaticPageExtractor
    ScrapingCoordinator --> EcommerceExtractor
    ScrapingCoordinator --> RealEstateExtractor
    DataHandler --> ScrapedData
    App --> ScrapingCoordinator
    App --> DataHandler

    Logger --> DynamicPageExtractor
    Logger --> RealEstateExtractor
    Logger --> DataHandler
    Logger --> ScrapingCoordinator

    Helpers --> EcommerceExtractor
    Helpers --> RealEstateExtractor
```

# ‚õÖ Relaciones en el Diagrama de Clases  

## 1. Herencia (Relaci√≥n "es un")  
### Descripci√≥n  
Indica que una clase es una especializaci√≥n de otra. La clase hija hereda atributos y m√©todos de la clase padre.  

### Relaciones  
- **WebDataExtractor ‚Üí StaticPageExtractor:**  
  - `StaticPageExtractor` es una especializaci√≥n de `WebDataExtractor` para manejar sitios est√°ticos (HTML/CSS).  
  - Hereda m√©todos como `download()`, `parse()` y `store()`, pero los implementa de manera espec√≠fica para p√°ginas est√°ticas.  
 
- **WebDataExtractor ‚Üí DynamicPageExtractor:**  
  - `DynamicPageExtractor` es una especializaci√≥n de `WebDataExtractor` para manejar sitios din√°micos (JavaScript).  
  - Hereda m√©todos como `download()`, `parse()` y `store()`, pero los implementa usando Selenium para interactuar con contenido din√°mico.  

- **DynamicPageExtractor ‚Üí EcommerceExtractor:**  
  - `EcommerceExtractor` es una especializaci√≥n de `DynamicPageExtractor` para manejar sitios de e-commerce (por ejemplo, MercadoLibre, Alkosto).  
  - Implementa m√©todos espec√≠ficos para extraer datos de productos, como precios, im√°genes y descripciones.  

- **DynamicPageExtractor ‚Üí RealEstateExtractor:**  
  - `RealEstateExtractor` es una especializaci√≥n de `DynamicPageExtractor` para manejar sitios de bienes ra√≠ces (por ejemplo, Metrocuadrado).  
  - Implementa m√©todos espec√≠ficos para extraer datos de propiedades, como precios, √°reas y ubicaciones.  

---

## 2. Composici√≥n (Relaci√≥n "tiene un")  
### Descripci√≥n  
Indica que una clase est√° compuesta por otras clases. La clase contenedora depende de las clases que la componen.  

### Relaciones  
- **EcommerceExtractor ‚Üí ProductData:**  
  - `EcommerceExtractor` utiliza `ProductData` para representar los datos de un producto (por ejemplo, t√≠tulo, precio, imagen).  
  - `ProductData` es una clase auxiliar que encapsula la estructura de los datos de un producto.  

- **EcommerceExtractor ‚Üí DataHandler:**  
  - `EcommerceExtractor` utiliza `DataHandler` para almacenar los datos extra√≠dos en JSON o SQL.  

- **RealEstateExtractor ‚Üí DataHandler:**  
  - `RealEstateExtractor` utiliza `DataHandler` para almacenar los datos extra√≠dos en JSON o SQL.  

- **ScrapingCoordinator ‚Üí StaticPageExtractor, EcommerceExtractor, RealEstateExtractor:**  
  - `ScrapingCoordinator` utiliza estas clases para ejecutar tareas de scraping.  
  - Coordina la ejecuci√≥n de m√∫ltiples tareas, seleccionando el extractor adecuado para cada URL.  

---

## 3. Asociaci√≥n (Relaci√≥n "usa")  
### Descripci√≥n  
Indica que una clase utiliza otra clase para realizar una tarea espec√≠fica, pero no hay una dependencia fuerte entre ellas.  

### Relaciones  
- **DataHandler ‚Üí ScrapedData:**  
  - `DataHandler` utiliza `ScrapedData` para almacenar los datos extra√≠dos en la base de datos.  
  - `ScrapedData` es un modelo de base de datos que representa los datos scrapeados.  

- **App ‚Üí ScrapingCoordinator:**  
  - `App` utiliza `ScrapingCoordinator` para gestionar las tareas de scraping.  
  - `ScrapingCoordinator` es responsable de ejecutar las tareas y devolver los resultados.  

- **App ‚Üí DataHandler:**  
  - `App` utiliza `DataHandler` para acceder a los datos almacenados y mostrarlos a trav√©s de la API.  

---

## 4. Dependencia de Utilidades (Relaci√≥n "usa")  
### Descripci√≥n  
Indica que una clase depende de una clase de utilidad para realizar tareas espec√≠ficas.  

### Relaciones  
- **DynamicPageExtractor, RealEstateExtractor, DataHandler, ScrapingCoordinator ‚Üí Logger:**  
  - Estas clases utilizan `Logger` para registrar eventos, errores y actividades durante la ejecuci√≥n del scraping.  
  - `Logger` es una clase de utilidad que centraliza la configuraci√≥n de logging.  

- **EcommerceExtractor, RealEstateExtractor ‚Üí Helpers:**  
  - Estas clases utilizan `Helpers` para realizar tareas comunes, como validar URLs, limpiar nombres de archivos y generar hashes.  
  - `Helpers` es una clase de utilidad que proporciona funciones auxiliares.  

---

## 5. Relaci√≥n entre ScrapedData y DataHandler  
### Descripci√≥n  
`DataHandler` utiliza `ScrapedData` para almacenar los datos extra√≠dos en la base de datos.  

### C√≥mo se afectan  
- `DataHandler` toma los datos extra√≠dos por los extractores (`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`) y los convierte en instancias de `ScrapedData`.  
- `ScrapedData` es un modelo de base de datos que define la estructura de los datos almacenados (por ejemplo, URL, tipo de datos, contenido, fecha de extracci√≥n).  

---

## 6. Relaci√≥n entre App y ScrapingCoordinator/DataHandler  
### Descripci√≥n  
`App` utiliza `ScrapingCoordinator` y `DataHandler` para gestionar la API y las tareas de scraping.  

### C√≥mo se afectan  
- `App` expone una API RESTful que permite a los usuarios iniciar tareas de scraping y consultar los datos almacenados.  
- `ScrapingCoordinator` ejecuta las tareas de scraping y devuelve los resultados a `App`.  
- `DataHandler` proporciona acceso a los datos almacenados, que `App` devuelve como respuestas de la API.  

---

# üíé **Utilidades**

## M√≥dulo `Logger`
El m√≥dulo `Logger` gestiona el registro de eventos en la aplicaci√≥n mediante el m√≥dulo `logging` de Python, permitiendo un rastreo flexible. Se integra con `os` para la gesti√≥n de directorios y `typing` para mejorar la claridad del c√≥digo.

### 1. Configuraci√≥n del Logger (`setup_logger`)
- **Prop√≥sito**: Configura el logger con handlers para archivo y consola.
- **Funcionalidades**:
  - Define el nivel de logging (`INFO`, `DEBUG`).
  - Elimina handlers duplicados.
  - Crea el directorio de logs si no existe.
  - Establece un formato est√°ndar para los logs.
  - Configura un handler de archivo rotativo (10 MB, 5 respaldos).
  - Agrega un handler de consola opcional.
  - Suprime logs de Selenium para evitar ruido.

```python
def setup_logger(config: Dict[str, Any]) -> logging.Logger:
    """Configura el logger ra√≠z con handlers para archivo y consola"""
    logger = logging.getLogger()
    logger.setLevel(config.get('level', 'INFO').upper())

    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    os.makedirs(config.get('log_dir', 'logs'), exist_ok=True)

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    file_handler = RotatingFileHandler(
        filename=os.path.join(config.get('log_dir', 'logs'), 'scraping.log'),
        maxBytes=10*1024*1024,  
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    if config.get('enable_console', True):
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
    logging.getLogger("selenium").setLevel(logging.WARNING)

    return logger
```
## 2. Obtenci√≥n del Logger (`get_logger`)

### Prop√≥sito
Retorna un logger configurado o uno b√°sico si `setup_logger` no se ha ejecutado.

### Funcionalidades
- Crea un logger b√°sico con handler de consola si no hay configuraciones previas.
- Establece `INFO` como nivel de logging predeterminado.

```python
def get_logger(name: str = None) -> logging.Logger:
    """Obtiene un logger configurado o un logger b√°sico si setup_logger no se ejecut√≥."""
    logger = logging.getLogger(name or "ScrapingLogger")

    if not logger.hasHandlers():
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)

    return logger
```

# **M√≥dulo `Helpers`**  

## **Descripci√≥n**
El m√≥dulo `Helpers` proporciona funciones auxiliares esenciales para el proyecto, incluyendo validaci√≥n de URLs, gesti√≥n de directorios, limpieza de nombres de archivos, c√°lculo de estad√≠sticas y generaci√≥n de hashes.  

## **L√≥gica del C√≥digo**  

### **1. Validaci√≥n de URLs (`validate_url`)**
- **Prop√≥sito**: Verifica si una URL tiene un formato v√°lido.
- **Funcionalidad**: Usa una expresi√≥n regular para validar el protocolo, dominio, puerto opcional y ruta opcional.

```python
def validate_url(url: str) -> bool:
    """Valida que una URL tenga formato correcto"""
    regex = re.compile(r'^(https?://)?(([A-Z0-9-]+\.)+[A-Z]{2,63})(:\d+)?(/.*)?$', re.IGNORECASE)
    return re.match(regex, url) is not None
```
## 2. Creaci√≥n de Directorios (`create_directory_structure`)

**Prop√≥sito**: Crea la estructura de directorios para almacenar los resultados del scraping.  
**Funcionalidad**: Usa `os.makedirs` para crear los directorios si no existen.  

```python
def create_directory_structure(base_path: str = "outputs") -> None:
    """Crea la estructura de directorios necesaria"""
    directories = [
        base_path,
        os.path.join(base_path, "static_pages_extractors"),
        os.path.join(base_path, "dynamic_extractors", "e-commerce"),
        os.path.join(base_path, "dynamic_extractors", "real_state"),
        "logs"
    ]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
```
## 3. Limpieza de Nombres de Archivos (`clean_filename`)

**Prop√≥sito**: Asegura que los nombres de archivos sean v√°lidos en todos los sistemas.  
**Funcionalidad**: Elimina caracteres especiales y espacios innecesarios.  

```python
def clean_filename(filename: str, max_length: int = 100) -> str:
    """Limpia un nombre de archivo para hacerlo v√°lido en todos los sistemas"""
    cleaned = re.sub(r'[\\/*?:"<>|]', "_", filename.strip())
    cleaned = re.sub(r'_{2,}', '_', cleaned)
    return cleaned[:max_length].strip('_')
```
## 4. C√°lculo de Estad√≠sticas (`calculate_stats`)

**Prop√≥sito**: Obtiene m√©tricas sobre los resultados del scraping.  
**Funcionalidad**:  
- Cuenta √©xitos y errores.  
- Calcula tasas de √©xito y error.  
- Agrupa errores por tipo.  

```python
def calculate_stats(results: List[Dict]) -> Dict[str, Union[int, float]]:
    """Calcula estad√≠sticas de los resultados del scraping"""
    stats = {'total': len(results), 'success': 0, 'errors': 0, 'error_types': {}, 'avg_time': 0.0}
    
    for result in results:
        if 'error' in result:
            stats['errors'] += 1
            error_type = result['error'].split(':')[0]
            stats['error_types'][error_type] = stats['error_types'].get(error_type, 0) + 1
        else:
            stats['success'] += 1

    if stats['total'] > 0:
        stats['success_rate'] = (stats['success'] / stats['total']) * 100
        stats['error_rate'] = (stats['errors'] / stats['total']) * 100
    
    return stats
```
## 5. Generaci√≥n de Hash (`generate_hash`)

**Prop√≥sito**: Crea un identificador √∫nico para un contenido.  
**Funcionalidad**: Usa el algoritmo MD5 y retorna los primeros `length` caracteres del hash.  

```python
def generate_hash(content: str, length: int = 8) -> str:
    """Genera un hash √∫nico para contenido"""
    return hashlib.md5(content.encode()).hexdigest()[:length]
```
# üìú**Clase Base**
## **WebDataExtractor**
La clase `WebDataExtractor` es una **clase abstracta** que define la interfaz y el flujo base para la extracci√≥n de datos de p√°ginas web. Establece un proceso est√°ndar para el scraping, que incluye la descarga del contenido HTML, el parseo de los datos y su almacenamiento. Las clases hijas (por ejemplo, `StaticPageExtractor` y `DynamicPageExtractor`) deben implementar los m√©todos abstractos definidos en esta clase.

## **Principios Aplicados**
### **1. Abstracci√≥n**
- Define m√©todos abstractos (`download`, `parse`, `store`) que deben ser implementados por las clases hijas.
- Esto asegura que todas las subclases sigan un flujo de trabajo com√∫n.

### **2. Herencia y Polimorfismo**
- Las subclases heredan de `WebDataExtractor` y pueden modificar o extender su comportamiento.
- Permite que diferentes tipos de extractores (est√°ticos, din√°micos) implementen su propia l√≥gica.

### **3. Encapsulamiento**
- Los atributos privados (`_url`, `_html_content`, `_data`) est√°n protegidos y solo se acceden a trav√©s de propiedades (`@property` y `@setter`).
- Esto garantiza que los procesos internos (descarga, parseo, almacenamiento) sean utilizados solo a trav√©s de la interfaz p√∫blica.

---

## **M√©todos Clave**

### **1. M√©todos Abstractos**
#### **`download(self)`**
- Descarga el contenido HTML de la URL.
- Debe ser implementado por las subclases (por ejemplo, usando `requests` para p√°ginas est√°ticas o `Selenium` para p√°ginas din√°micas).

#### **`parse(self)`**
- Parsea el contenido HTML descargado y extrae la informaci√≥n deseada.
- Debe devolver una estructura de datos (por ejemplo, lista o diccionario).

#### **`store(self)`**
- Almacena los datos extra√≠dos en un formato espec√≠fico (por ejemplo, JSON o base de datos).
- Debe devolver `True` si el almacenamiento fue exitoso, de lo contrario, `False`.

---

### **2. M√©todos Concretos**
#### **`scrape(self)`**
- Orquesta el proceso completo de scraping:
  1. Descarga el contenido HTML.
  2. Parsea el contenido para extraer datos.
  3. Almacena los datos extra√≠dos.
- Devuelve una lista de datos o `None` si ocurre un error.

#### **`iter_data(self)`**
- M√©todo generador que permite iterar sobre los datos extra√≠dos de forma eficiente.
- √ötil para manejar grandes vol√∫menes de datos sin cargar toda la lista en memoria.

---

## **Atributos Clave**
- **`_url`**:
  - URL a scrapear (privado).
  - Accesible a trav√©s de la propiedad `url`.

- **`_html_content`**:
  - Contenido HTML descargado (privado).
  - Accesible a trav√©s de la propiedad `html_content`.

- **`_data`**:
  - Datos extra√≠dos (privado).
  - Accesible a trav√©s de la propiedad `data`.

- **`logger`**:
  - Configuraci√≥n de logging para registrar eventos y errores.

---

## **Ejemplo de Uso**
```python
class StaticPageExtractor(WebDataExtractor):
    def download(self):
        # Implementaci√≥n espec√≠fica para p√°ginas est√°ticas
        pass

    def parse(self):
        # Implementaci√≥n espec√≠fica para p√°ginas est√°ticas
        pass

    def store(self):
        # Implementaci√≥n espec√≠fica para p√°ginas est√°ticas
        pass

extractor = StaticPageExtractor("https://example.com")
data = extractor.scrape()  # Ejecuta el proceso completo de scraping
```
# üõ∏ **Extractores**
## **`static_page_extractor.py`**
Implementa un extractor para p√°ginas web est√°ticas, heredando de la clase abstracta `WebDataExtractor`. Este m√≥dulo se encarga de:
- **Descargar el contenido HTML** de p√°ginas est√°ticas usando `requests`, con gesti√≥n de cach√© y reintentos (ver m√©todo `download()`).
- **Parsear el HTML** para extraer informaci√≥n estructurada (t√≠tulo, infobox, contenido, im√°genes, listas y tablas) usando `BeautifulSoup` (ver m√©todo `parse()`).
- **Almacenar los datos extra√≠dos** en una base de datos SQL mediante `SQLAlchemy` (ver m√©todo `store()`).
- **Personalizar selectores** y par√°metros a trav√©s de un archivo de configuraci√≥n (ver atributo `_selectores` y m√©todo `get_selectores()`).

## **Principios de POO**
- **Herencia y Polimorfismo**: Hereda de `WebDataExtractor` e implementa sus m√©todos abstractos (`download`, `parse`, `store`).
- **Encapsulamiento**: La l√≥gica interna (descarga, parseo y almacenamiento) est√° oculta, exponi√©ndose √∫nicamente a trav√©s de la interfaz p√∫blica (ver m√©todos p√∫blicos y propiedades).


## **Dependencias**
- **M√≥dulos principales**:
  - `hashlib`: Para generar hashes √∫nicos (ver m√©todo `get_cache_filename()`).
  - `json`: Para manejar datos en formato JSON (ver m√©todo `store()`).
  - `os`: Para interactuar con el sistema de archivos (ver m√©todo `get_cache_filename()`).
  - `requests`: Para realizar solicitudes HTTP (ver m√©todo `download()`).
  - `time`: Para manejar tiempos de espera (ver m√©todo `download()`).
  - `BeautifulSoup`: Para parsear HTML (ver m√©todo `parse()`).
  - `urllib.parse.urljoin`: Para construir URLs absolutas (ver m√©todo `parse()`).



## **Clase Principal: `StaticPageExtractor`**
### **Atributos**
- **`_selectores`**: Selectores espec√≠ficos para p√°ginas est√°ticas (ver m√©todo `get_selectores()`).

### **M√©todos**
- **`download()`**: Descarga el contenido HTML con gesti√≥n de cach√© y reintentos (ver implementaci√≥n).
- **`parse()`**: Parsea el HTML para extraer informaci√≥n estructurada (ver implementaci√≥n).
- **`store()`**: Almacena los datos extra√≠dos en JSON o SQL (ver implementaci√≥n).
- **`get_selectores()`**: Obtiene los selectores seg√∫n el dominio de la URL (ver implementaci√≥n).
- **`get_cache_filename()`**: Genera un nombre de archivo √∫nico para la cach√© (ver implementaci√≥n).

## **Ejemplo de Uso**
```python
extractor = StaticPageExtractor("https://es.wikipedia.org/wiki/Python")
data = extractor.scrape()  # Descarga y parsea el contenido
extractor.store()  # Almacena los datos en JSON/SQL
```
## **Pruebas**
El m√≥dulo incluye pruebas unitarias para verificar su funcionamiento con URLs de Wikipedia y Fandom (ver bloque `if __name__ == "__main__":`).


## **Fragmentos de C√≥digo Destacados**

### **1. Descarga con Cach√©**
```python
def download(self):
    cache_file = self.get_cache_filename()
    if self.url in CACHE:
        return CACHE[self.url]
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            return f.read()
    # L√≥gica de descarga y almacenamiento en cach√©...
```
### **2. Descarga con Cach√©** 
```python def parse(self):
    soup = BeautifulSoup(self.html_content, 'html.parser')
    title = soup.find("h1").get_text(strip=True)  # Extrae el t√≠tulo
    infobox = self._extraer_infobox(soup)  # Extrae la infobox
    content = self._extraer_contenido(soup)  # Extrae el contenido
    return {"title": title, "infobox": infobox, "content": content}
 ```
### **3. Descarga con Cach√©** 
```python def store(self):
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="static")
```
## **M√≥dulo: `dynamic_page_extractor.py`**
Implementa un extractor para p√°ginas web din√°micas, heredando de la clase abstracta `WebDataExtractor`. Este m√≥dulo se encarga de:
- **Cargar p√°ginas din√°micas** usando `Selenium WebDriver` en modo headless (ver m√©todo `download()`).
- **Esperar a que se renderice el contenido** din√°mico (ver uso de `WebDriverWait` en `download()`).
- **Parsear el HTML** resultante usando `BeautifulSoup` para extraer datos (ver m√©todo `parse()`).
- **Almacenar los datos extra√≠dos** en un archivo JSON o en una base de datos SQL (ver m√©todo `save_store()`).

---

### **Principios de POO**
- **Herencia**: Hereda de `WebDataExtractor` e implementa sus m√©todos abstractos (`download`, `parse`, `store`).
- **Polimorfismo**: Implementa m√©todos espec√≠ficos para p√°ginas din√°micas.
- **Encapsulamiento**: La l√≥gica interna (carga, espera, parseo y almacenamiento) est√° encapsulada en m√©todos privados.

---

### **Dependencias**
- **`selenium`**: Para automatizar la interacci√≥n con navegadores web (ver m√©todo `download()`).
- **`BeautifulSoup`**: Para parsear HTML (ver m√©todo `parse()`).
- **`random`**: Para seleccionar un agente de usuario aleatorio (ver inicializaci√≥n de `USER_AGENT`).
- **`time`**: Para manejar tiempos de espera (ver m√©todo `download()`).
- **`urllib.parse`**: Para trabajar con URLs (ver m√©todo `detectar_tienda()`).

---

### **Clase Principal: `DynamicPageExtractor`**
#### **Atributos**
- `_tienda`: Tipo de tienda detectada (e.g., "mercadolibre", "alkosto") (ver m√©todo `detectar_tienda()`).
- `_num_productos`: N√∫mero de productos a extraer (ver propiedad `num_productos`).
- `driver`: Instancia de Selenium WebDriver (ver m√©todo `configurar_driver()`).

#### **M√©todos**
- `download()`: Descarga el contenido din√°mico usando Selenium (ver implementaci√≥n).
- `parse()`: Parsea el HTML din√°mico (abstracto, implementado en subclases).
- `save_store()`: Almacena los datos extra√≠dos en JSON o SQL (ver implementaci√≥n).
- `detectar_tienda()`: Detecta la tienda basada en el dominio de la URL (ver implementaci√≥n).
- `configurar_driver()`: Configura el WebDriver de Selenium (ver implementaci√≥n).

---

### **Ejemplo de Uso**
```python
extractor = DynamicPageExtractor("https://www.mercadolibre.com.co")
extractor.download()  # Descarga el contenido din√°mico
data = extractor.parse()  # Parsea el HTML
extractor.save_store()  # Almacena los datos
```
### **Descarga con Selenium**
```python 
def download(self):
    opciones = Options()
    opciones.add_argument("--headless=new")  # Modo headless
    opciones.add_argument(f"user-agent={random.choice(USER_AGENT_DINAMICOS)}")
    driver = webdriver.Chrome(options=opciones)
    driver.get(self.url)
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    return driver.page_source
```

### **Detecci√≥n de Tienda**
```python
def detectar_tienda(self):
    dominio = urlparse(self.url).netloc.lower()
    for tienda, dominios in dominios_tiendas.items():
        if any(subdominio in dominio for subdominio in dominios):
            return tienda
    raise ValueError(f"No se reconoce la tienda para el dominio: {dominio}")
```
### **Almacenamiento de Datos**
```python   def save_store(self):
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="dynamic")
```
## **M√≥dulo: `ecommerce_extractor.py`**
Implementa un extractor de datos para p√°ginas web din√°micas de e-commerce, como MercadoLibre y Alkosto. Utiliza `Selenium` para renderizar el contenido din√°mico y `BeautifulSoup` para parsear el HTML. Este m√≥dulo permite extraer informaci√≥n estructurada de productos, como t√≠tulos, precios, im√°genes, descuentos y descripciones.

### **Caracter√≠sticas Principales**
- **Paginaci√≥n autom√°tica**: Navega por m√∫ltiples p√°ginas de resultados.
- **Manejo de errores robusto**: Reintentos y logging detallado.
- **Almacenamiento flexible**: Guarda datos en JSON o base de datos SQL.

---

### **Principios de POO**
- **Herencia**: Hereda de `DynamicPageExtractor`.
- **Composici√≥n**: Utiliza la clase `ProductData` para estructurar los datos de productos.
- **Encapsulamiento**: La l√≥gica de extracci√≥n y almacenamiento est√° encapsulada en m√©todos.

---

### **Dependencias**
- **`selenium`**: Para interactuar con p√°ginas din√°micas (ver m√©todo `download()`).
- **`BeautifulSoup`**: Para parsear HTML (ver m√©todo `parse()`).
- **`re`**: Para expresiones regulares (ver m√©todo `extraer_puntuacion()`).
- **`urllib.parse`**: Para manejar URLs (ver m√©todo `extraer_url()`).
- **`typing`**: Para definir tipos de datos (ver atributos de `ProductData`).

---

### **Clases Principales**

#### **1. `ProductData`**
- **Atributos**:
  - `title`: T√≠tulo del producto.
  - `image`: URL de la imagen.
  - `price_original`: Precio original.
  - `price_sell`: Precio de venta.
  - `discount`: Descuento aplicado.
  - `rating`: Calificaci√≥n y rese√±as.
  - `url`: URL del producto.
  - `description`: Descripci√≥n del producto.
- **M√©todos**:
  - `to_dict()`: Convierte los datos a un diccionario.

#### **2. `EcommerceExtractor`**
- **Atributos**:
  - `tienda`: Tipo de tienda (e.g., "mercadolibre", "alkosto").
  - `num_productos`: N√∫mero de productos a extraer.
- **M√©todos**:
  - `parse()`: Extrae y estructura datos de productos (ver implementaci√≥n).
  - `extraer_texto()`: Extrae texto de un elemento.
  - `extraer_imagen()`: Extrae URL de la imagen.
  - `extraer_precio()`: Extrae y formatea el precio.
  - `procesar_descuento()`: Extrae el porcentaje de descuento.
  - `extraer_puntuacion()`: Extrae la calificaci√≥n del producto.
  - `extraer_url()`: Construye URL absoluta.
  - `extraer_descripcion()`: Extrae la descripci√≥n del producto.

---

### **Ejemplo de Uso**
```python
extractor = EcommerceExtractor("https://www.mercadolibre.com.co", tienda="mercadolibre", num_productos=5)
extractor.download()  # Descarga el contenido din√°mico
data = extractor.parse()  # Parsea los datos de productos
extractor.store()  # Almacena los datos en JSON/SQL
```
## **Extracci√≥n de Precio**
```python
def extraer_precio(self, elemento: Tag, selector_padre: Dict) -> str:
    if not selector_padre:
        return "Precio no disponible"
    elemento_padre = elemento.find(selector_padre["tag"], class_=selector_padre.get("class"))
    if not elemento_padre:
        return "Precio no encontrado"
    return elemento_padre.get_text(strip=True)
```
## **Extracci√≥n de Imagen**
```python
def extraer_imagen(self, elemento: Tag, selector: Dict) -> Union[str, None]:
    contenedor = elemento.find(selector["tag"], class_=selector.get("class"))
    if not contenedor:
        return None
    img_element = contenedor.find("img")
    return img_element["src"] if img_element else None
```
## **Almacenamiento de Datos**
```python
def store(self) -> bool:
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="e-commerce")
```
## **M√≥dulo: `real_estate_extractor.py`**
Extractor de datos para p√°ginas web din√°micas de bienes ra√≠ces (ejemplo: Metrocuadrado). Utiliza `Selenium` para renderizar contenido din√°mico y `BeautifulSoup` para parsear HTML. Extrae informaci√≥n de propiedades como t√≠tulo, precio, √°rea, habitaciones, ba√±os y URL.

---

### **Caracter√≠sticas**
- **Paginaci√≥n autom√°tica**: Navega por m√∫ltiples p√°ginas de resultados.
- **Manejo de errores robusto**: Reintentos y logging detallado.
- **Almacenamiento flexible**: Guarda datos en JSON o base de datos SQL.

---

### **Dependencias**
- **M√≥dulos principales**:
  - `selenium`: Para interactuar con p√°ginas din√°micas (ver m√©todo `download()`).
  - `BeautifulSoup`: Para parsear HTML (ver m√©todo `parse()`).
  - `re`: Para expresiones regulares (ver m√©todo `extraer_precio()`).
  - `urllib.parse`: Para manejar URLs (ver m√©todo `extraer_url()`).
  - `typing`: Para definir tipos de datos (ver atributos de la clase).

---

### **Clase Principal: `RealEstateExtractor`**
- **Atributos**:
  - `tienda`: Tipo de tienda (e.g., "metrocuadrado").
  - `num_productos`: N√∫mero de propiedades a extraer.
  - `driver`: Instancia de Selenium WebDriver.
- **M√©todos**:
  - `download()`: Descarga el contenido din√°mico usando Selenium.
  - `parse()`: Extrae y estructura datos de propiedades.
  - `store()`: Almacena los datos en JSON o SQL.
  - `extraer_titulo()`, `extraer_precio()`, `extraer_url()`, etc.: M√©todos espec√≠ficos para extraer datos.

---

### **Ejemplo de Uso**
```python
extractor = RealEstateExtractor("https://www.metrocuadrado.com", num_productos=5)
extractor.download()  # Descarga el contenido
data = extractor.parse()  # Parsea los datos
extractor.store()  # Almacena en JSON/SQL
```
## **Descarga con Selenium**
```python
def download(self):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    driver = webdriver.Chrome(options=options)
    driver.get(self.url)
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    return driver.page_source
```

## **Extracci√≥n de Precio**
```python
def extraer_precio(self, elemento: Tag, selector: Dict) -> str:
    elemento_padre = elemento.find(selector["tag"], class_=selector.get("class"))
    return elemento_padre.get_text(strip=True) if elemento_padre else "Precio no disponible"
```

## **Almacenamiento de Datos**
```python
def store(self) -> bool:
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="real_state")
```
## **M√≥dulo: `RealEstateExtractor`**
Implementa un extractor para p√°ginas web din√°micas de bienes ra√≠ces, heredando de `DynamicPageExtractor`. Este m√≥dulo se encarga de:
- **Cargar p√°ginas din√°micas** de portales inmobiliarios (e.g., Metrocuadrado) usando `Selenium WebDriver` (ver m√©todo `download()`).
- **Extraer datos de propiedades** como t√≠tulo, precio, √°rea, habitaciones y ba√±os (ver m√©todo `parse()`).
- **Almacenar los datos extra√≠dos** en JSON o SQL (ver m√©todo `store()`).

---

### **Principios de POO**
- **Herencia**: Hereda de `DynamicPageExtractor` y extiende su funcionalidad para bienes ra√≠ces.
- **Encapsulamiento**: La l√≥gica de configuraci√≥n del navegador, extracci√≥n de datos y almacenamiento est√° encapsulada en m√©todos privados.
- **Reutilizaci√≥n**: Utiliza `DataHandler` para almacenar datos y `BeautifulSoup` para parsear HTML.

---

### **Dependencias**
- **M√≥dulos principales**:
  - `selenium`: Para automatizar la interacci√≥n con navegadores web (ver m√©todo `download()`).
  - `BeautifulSoup`: Para parsear HTML (ver m√©todo `parse()`).
  - `re`: Para trabajar con expresiones regulares (ver m√©todo `extraer_ubicacion_url()`).
  - `logging`: Para registrar eventos y errores (ver inicializaci√≥n del `logger`).

---

### **Clase Principal: `RealEstateExtractor`**
- **Atributos**:
  - `driver`: Instancia de Selenium WebDriver (ver m√©todo `configurar_driver()`).
  - `ubicacion`: Ubicaci√≥n extra√≠da de la URL (ver m√©todo `extraer_ubicacion_url()`).
  - `user_agent`: Agente de usuario aleatorio (ver inicializaci√≥n).

- **M√©todos**:
  - `download()`: Descarga el contenido din√°mico y maneja la paginaci√≥n (ver implementaci√≥n).
  - `parse()`: Extrae datos de propiedades inmobiliarias (ver implementaci√≥n).
  - `store()`: Almacena los datos extra√≠dos en JSON o SQL (ver implementaci√≥n).
  - `configurar_tipo_negocio()`: Configura el tipo de negocio (compra/venta) (ver implementaci√≥n).
  - `configurar_ubicacion_exacta()`: Configura la ubicaci√≥n exacta de la propiedad (ver implementaci√≥n).
  - `configurar_tipo_inmueble()`: Selecciona tipos de inmueble (e.g., apartamentos, casas) (ver implementaci√≥n).

---

### **Ejemplo de Uso**
```python
extractor = RealEstateExtractor(
    url="https://www.metrocuadrado.com/buscar/",
    num_productos=3,
    tipos=["Apartamentos"],
    ciudad="Bogot√°",
    localidad="Chapinero"
)
extractor.download()  # Descarga el contenido din√°mico
data = extractor.parse()  # Extrae datos de propiedades
extractor.store()  # Almacena los datos
```
## **Descarga con Paginaci√≥n**
``` Python
def download(self):
    while len(propiedades_unicas) < self.num_productos:
        html_page = self.driver.page_source
        nuevas_props = self.parse(html_page)
        if not self.ir_pagina_siguiente(pagina_actual):
            break
        pagina_actual += 1

```
## **Extracci√≥n de Datos**
``` Python
def parse(self):
    propiedades = []
    for prop in todos_listados:
        titulo = self.extraer_titulo(prop, config['children']['title'])
        precio = self.extraer_precio(prop, config['children']['price'])
        propiedades.append({
            "title": titulo,
            "price": precio,
            "url": self.extraer_url(prop, config['children']['url'])
        })
    return propiedade
```
## **Configuraci√≥n de Ubicaci√≥n**
``` Python
def configurar_ubicacion_exacta(self):
    input_loc = self.driver.find_element(By.CSS_SELECTOR, 'input[name="location"]')
    input_loc.send_keys(self.params['localidad'])
    WebDriverWait(self.driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "div.react-autosuggest__suggestions-container li:first-child"))
    ).click()
```

# üìã Manejo de Datos
## **M√≥dulo: `DataHandler`**
Clase encargada del manejo y procesamiento de datos extra√≠dos durante el proceso de scraping. Proporciona funcionalidades para:
- **Almacenar datos** en formato JSON o SQL (ver m√©todos `store_json()` y `store_sql()`).
- **Generar reportes** en formato TXT o HTML (ver m√©todo `generate_report()`).
- **Categorizar datos** extra√≠dos (ver m√©todo `categorize_data()`).

---

### **Principios de POO**
- **Encapsulamiento**: Los datos y la l√≥gica de almacenamiento est√°n encapsulados en m√©todos privados.
- **Reutilizaci√≥n**: Utiliza `ScrapedData` para almacenar datos en la base de datos y `json` para manejar archivos JSON.
- **Flexibilidad**: Soporta m√∫ltiples formatos de almacenamiento (`json`, `sql`, `both`).

---

### **Dependencias**
- **M√≥dulos principales**:
  - `json`: Para convertir datos a formato JSON (ver m√©todo `store_json()`).
  - `hashlib`: Para generar hashes √∫nicos (ver m√©todo `store_json()`).
  - `os`: Para manejar directorios y archivos (ver m√©todo `store_json()`).
  - `logging`: Para registrar eventos y errores (ver inicializaci√≥n del `logger`).
  - `ScrapedData`: Modelo de base de datos para almacenar datos scrapeados (ver m√©todo `store_sql()`).

---

### **Clase Principal: `DataHandler`**
- **Atributos**:
  - `_data`: Datos a manejar (privado).
  - `_storage_format`: Formato de almacenamiento (`json`, `sql`, `both`) (privado).
  - `_logger`: Configuraci√≥n de logging (privado).

- **M√©todos**:
  - `store_data()`: Almacena los datos en el formato especificado (ver implementaci√≥n).
  - `store_json()`: Almacena los datos en archivos JSON (ver implementaci√≥n).
  - `store_sql()`: Almacena los datos en una base de datos SQL (ver implementaci√≥n).
  - `generate_report()`: Genera un reporte en formato TXT o HTML (ver implementaci√≥n).
  - `categorize_data()`: Categoriza los datos extra√≠dos (ver implementaci√≥n).

---

### **Ejemplo de Uso**
```python
data = [{"title": "Propiedad 1", "price": "$100,000"}]
handler = DataHandler(data, storage_format='both')
handler.store_data(url="https://example.com", tipo="real_state")
handler.generate_report(report_type='html')
```

---

## **Fragmentos de C√≥digo Destacados**

### **1. Almacenamiento en JSON**
```python
def store_json(self, url: str, tipo: str) -> bool:
    output_dir = os.path.join("outputs", "dynamic_extractors/real_state")
    os.makedirs(output_dir, exist_ok=True)
    filename = f"data_{hashlib.md5(url.encode()).hexdigest()[:8]}.json"
    with open(os.path.join(output_dir, filename), "w", encoding="utf-8") as f:
        json.dump(self.data, f, ensure_ascii=False, indent=4)
    return True
```

---

### **2. Almacenamiento en SQL**
```python
def store_sql(self, tipo: str) -> bool:
    session = SessionLocal()
    for item in self.data:
        new_record = ScrapedData(
            url=item.get("url"),
            tipo=tipo,
            contenido=json.dumps(item, ensure_ascii=False)
        )
        session.add(new_record)
    session.commit()
    return True
```

---

### **3. Generaci√≥n de Reportes**
```python
def generate_report(self, report_type='txt'):
    filename = os.path.join("outputs/reports", f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{report_type}")
    if report_type == 'txt':
        with open(filename, "w", encoding="utf-8") as f:
            f.write("Reporte de Datos Extra√≠dos\n")
            for item in self.data:
                f.write(f"{item}\n")
    return filename
```
### M√≥dulo: `ScrapedData` (Modelo de Base de Datos)
Define la estructura de la tabla `scraped_data` en la base de datos utilizando SQLAlchemy. Esta tabla almacena los datos extra√≠dos durante el proceso de scraping, incluyendo la URL, el tipo de datos, el contenido y la fecha de extracci√≥n.

---

### Dependencias
- M√≥dulos principales:
  - `sqlalchemy`: Para definir modelos de base de datos y realizar operaciones SQL (ver uso de `Column`, `Integer`, `String`, `DateTime`).
  - `datetime`: Para manejar fechas y horas (ver campo `fecha_extraccion`).

### Clase Principal: `ScrapedData`
- **Atributos**:
  - `id`: Identificador √∫nico de cada registro (clave primaria).
  - `url`: URL de la cual se extrajeron los datos (cadena de hasta 500 caracteres).
  - `tipo`: Tipo de datos extra√≠dos (e.g., "static", "e-commerce", "real_state") (cadena de hasta 50 caracteres).
  - `contenido`: Datos extra√≠dos en formato JSON (cadena de texto).
  - `fecha_extraccion`: Fecha y hora de la extracci√≥n (se establece autom√°ticamente al crear el registro).

### Ejemplo de Uso
```python
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine

# Crear una sesi√≥n de base de datos
engine = create_engine("sqlite:///scraping_data.db")
Session = sessionmaker(bind=engine)
session = Session()

# Crear un nuevo registro
nuevo_registro = ScrapedData(
    url="https://example.com",
    tipo="static",
    contenido='{"title": "Ejemplo", "content": "Lorem ipsum"}'
)
session.add(nuevo_registro)
session.commit()
```

### Fragmentos de C√≥digo Destacados

1. **Definici√≥n del Modelo**:
   ```python
   class ScrapedData(Base):
       __tablename__ = "scraped_data"
       id = Column(Integer, primary_key=True)
       url = Column(String(500), nullable=False)
       tipo = Column(String(50), nullable=False)
       contenido = Column(String, nullable=False)
       fecha_extraccion = Column(DateTime, default=datetime.utcnow)
   ```
---
2. **Uso de `datetime.utcnow`**:
   ```python
   fecha_extraccion = Column(DateTime, default=datetime.utcnow)
   ```
   Este campo se llena autom√°ticamente con la fecha y hora actuales en formato UTC al crear un nuevo registro.

---

## Relaci√≥n con Otros M√≥dulos
- **`DataHandler`**: Utiliza `ScrapedData` para almacenar datos en la base de datos (ver m√©todo `store_sql()` en `DataHandler`).
- **`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`**: Dependen de `ScrapedData` para persistir los datos extra√≠dos.
---
# üéÆCoordinacioÃÅn
## M√≥dulo: `ScrapingCoordinator`
Clase que coordina el scraping de m√∫ltiples tareas, gestionando tanto p√°ginas est√°ticas como din√°micas (e-commerce y bienes ra√≠ces). Sus principales funciones son:
- **Validar tareas**: Asegura que cada tarea tenga los campos necesarios (ver m√©todo `validate_tasks()`).
- **Seleccionar extractores**: Elige el extractor adecuado seg√∫n el tipo de tarea (ver m√©todo `select_extractor()`).
- **Ejecutar tareas**: Procesa las tareas de manera concurrente usando `ThreadPoolExecutor` (ver m√©todo `run()`).
- **Generar estad√≠sticas**: Proporciona un resumen del proceso de scraping (ver m√©todo `run()`).
---
### Principios de POO
- **Encapsulamiento**: La l√≥gica de validaci√≥n, selecci√≥n de extractores y ejecuci√≥n de tareas est√° encapsulada en m√©todos privados.
- **Concurrencia**: Utiliza `ThreadPoolExecutor` para ejecutar tareas en paralelo, mejorando la eficiencia.
- **Flexibilidad**: Soporta m√∫ltiples tipos de tareas (est√°ticas y din√°micas) y subtipos (e-commerce, real_state).
---
### Dependencias
- M√≥dulos principales:
  - `concurrent.futures`: Para ejecutar tareas de manera concurrente (ver m√©todo `run()`).
  - `logging`: Para registrar eventos y errores (ver inicializaci√≥n del `logger`).
  - `StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`: Extractores espec√≠ficos para cada tipo de tarea (ver m√©todo `select_extractor()`).
---
### Clase Principal: `ScrapingCoordinator`
- **Atributos**:
  - `tasks`: Lista de tareas a ejecutar.
  - `max_workers`: N√∫mero m√°ximo de hilos para ejecuci√≥n concurrente.
  - `results`: Resultados del scraping.
- **M√©todos**:
  - `validate_tasks()`: Valida la estructura de las tareas (ver implementaci√≥n).
  - `select_extractor()`: Selecciona el extractor adecuado para cada tarea (ver implementaci√≥n).
  - `process_task()`: Ejecuta una tarea de scraping (ver implementaci√≥n).
  - `run()`: Orquesta la ejecuci√≥n de todas las tareas y genera estad√≠sticas (ver implementaci√≥n).
---
### Ejemplo de Uso
```python
tasks = [
    {"url": "https://example.com/static", "type": "static"},
    {"url": "https://example.com/ecommerce", "type": "dynamic", "subtype": "e-commerce"},
    {"url": "https://example.com/realstate", "type": "dynamic", "subtype": "real_state"}
]

coordinator = ScrapingCoordinator(tasks, max_workers=3)
resultados = coordinator.run()
print(resultados['statistics'])  # Muestra estad√≠sticas del scraping
```
---
### Fragmentos de C√≥digo Destacados

1. **Validaci√≥n de Tareas**:
   ```python
   def validate_tasks(self, tasks):
       for task in tasks:
           if task['type'] not in ['static', 'dynamic']:
               raise ValueError(f"Tipo de tarea inv√°lido: {task['type']}")
   ```
---
2. **Selecci√≥n de Extractor**:
   ```python
   def select_extractor(self, task):
       if task['type'] == 'static':
           return StaticPageExtractor(task['url'])
       elif task['subtype'] == 'e-commerce':
           return EcommerceExtractor(task['url'])
       elif task['subtype'] == 'real_state':
           return RealEstateExtractor(task['url'])
   ```
---
3. **Ejecuci√≥n Concurrente**:
   ```python
   def run(self):
       with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
           futures = [executor.submit(self.process_task, task) for task in self.tasks]
           for future in as_completed(futures):
               self.results.append(future.result())
   ```
---
4. **Generaci√≥n de Estad√≠sticas**:
   ```python
   stats = {
       'total_tasks': len(self.tasks),
       'success': sum(1 for r in self.results if 'data' in r),
       'errors': len(self.results) - success,
       'error_rate': f"{(errors/len(self.tasks)*100):.1f}%"
   }
   ```
---
### Relaci√≥n con Otros M√≥dulos
- **`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`**: Utilizados para ejecutar tareas espec√≠ficas.
- **`DataHandler`**: Puede ser utilizado para almacenar los resultados del scraping.

# üçµ Soporte
### **Clase: `ProductData`**
Clase auxiliar que encapsula la estructura de datos de un producto. Define los atributos b√°sicos de un producto, como t√≠tulo, imagen, precios, descuento, calificaci√≥n, URL y descripci√≥n. Tambi√©n incluye un m√©todo para convertir los datos del producto en un diccionario.

---

### **Principios de POO**
- **Encapsulamiento**: Los atributos del producto est√°n encapsulados en la clase.
- **Simplicidad**: Proporciona una estructura clara y f√°cil de usar para representar productos.
- **Reutilizaci√≥n**: Puede ser utilizada por otras clases como `EcommerceExtractor` para manejar datos de productos.
---
### **Atributos**
- `title`: T√≠tulo del producto (cadena de texto).
- `image`: URL de la imagen del producto (cadena de texto o `None`).
- `price_original`: Precio original del producto (cadena de texto).
- `price_sell`: Precio de venta del producto (cadena de texto).
- `discount`: Descuento aplicado al producto (cadena de texto, valor por defecto: `"0%"`).
- `rating`: Calificaci√≥n y n√∫mero de rese√±as del producto (diccionario).
- `url`: URL del producto (cadena de texto).
- `description`: Descripci√≥n del producto (cadena de texto o `None`).
---
### **M√©todos**
- `to_dict()`: Convierte los datos del producto en un diccionario (ver implementaci√≥n).
---
### **Ejemplo de Uso**
```python
producto = ProductData()
producto.title = "Producto Ejemplo"
producto.image = "https://example.com/image.jpg"
producto.price_sell = "$100"
producto.url = "https://example.com/producto"

# Convertir a diccionario
datos_producto = producto.to_dict()
print(datos_producto)
```
---
### **Fragmento de C√≥digo Destacado**

1. **Conversi√≥n a Diccionario**:
   ```python
   def to_dict(self) -> Dict:
       return self.__dict__
   ```

---

### **Relaci√≥n con Otros M√≥dulos**
- **`EcommerceExtractor`**: Utiliza `ProductData` para representar los datos de productos extra√≠dos.
- **`DataHandler`**: Puede utilizar `ProductData` para almacenar datos en JSON o SQL.
