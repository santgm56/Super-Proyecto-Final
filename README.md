# üåê Sistema WebScrapping

## üóíÔ∏è Colaboradores

```
‚îú‚îÄ‚îÄ Santiago Gamboa Mart√≠nez
‚îú‚îÄ‚îÄ Samuel Eduardo Fajardo Quintero
‚îî‚îÄ‚îÄ Manuel Felipe Torres Gamboa
```

# üèÜ Introducci√≥n

El volumen de informaci√≥n que se encuentra disponible en internet crece de manera exponencial, haciendo indispensable el uso de herramientas tecnol√≥gicas que permitan extraer y analizar datos relevantes de forma autom√°tica y eficiente. Por esta raz√≥n, como equipo, hemos elegido desarrollar la `alternativa 2`: **_Sistema de WebScrapping_**, este proyecto consiste en desarrollar e implementar un sistema de web scraping que no solo cumpla con los objetivos de extracci√≥n de datos, sino que tambi√©n est√© estructurado bajo los principios fundamentales de la Programaci√≥n Orientada a Objetos (POO).

Este proyecto, titulado: "Super_Proyecto_Final", tiene como objetivo dise√±ar un sistema de web scraping estructurado bajo principios de modularidad y escalabilidad. Para ello, se emplear√°n herramientas como Python junto a librer√≠as especializadas como Requests, BeautifulSoup, Selenium y Pandas. Adem√°s, se garantizar√° un desarrollo robusto mediante buenas pr√°cticas, como el manejo adecuado de excepciones y una organizaci√≥n eficiente del c√≥digo estructurado bajo el paradigma de Programaci√≥n Orientada a Objetos (POO).

# ‚ûï Definici√≥n de Alternativa

La alternativa para este proyecto consiste en el desarrollo de un sistema de web scraping que emplee como pilar principal la Programaci√≥n Orientada a Objetos (POO). El sistema, como ya se mencion√≥ anteriormente, ser√° desarrollado en Python, un lenguaje ampliamente reconocido por su versatilidad y su extenso ecosistema de librer√≠as dise√±adas para la extracci√≥n y manipulaci√≥n de datos desde la web, ademas, se contar√° con la implementaci√≥n de un entorno virtual en el cual se instalar√°n las dependencias necesarias para desarrollar y ejecutar este sistema de web scraping.


### Ventajas de esta alternativa:

- Facilita la organizaci√≥n y escalabilidad del sistema gracias a la implementaci√≥n de Programaci√≥n Orientada a Objetos (POO).
- Aprovecha el amplio ecosistema y la versatilidad de Python, que incluye librer√≠as robustas que a su vez estan bien documentadas.
- Brinda flexibilidad para adaptarse a diversas necesidades, como la extracci√≥n de datos est√°ticos o din√°micos dependiendo el caso.
- Fomenta la adquisici√≥n de habilidades de dise√±o y codificaci√≥n para su aplicaci√≥n en escenarios reales.

# üóÇÔ∏è Requerimientos T√©cnicos:
 
### 1. **Lenguaje y Librer√≠as**:

- Python como lenguaje principal.

 ### **Librer√≠as**:
- **`Requests`** para realizar solicitudes HTTP.
- **`BeautifulSoup`** para parsear y extraer datos de HTML.

- **`Selenium`** para interactuar con p√°ginas din√°micas.
- **`Pandas`** para almacenar y procesar datos en estructuras organizadas.

### 2. **Estructura del C√≥digo**:

Implementaci√≥n bajo los principios de la Programaci√≥n Orientada a Objetos (POO) para garantizar modularidad y escalabilidad.

### **Clases principales**:
- **`WebDataExtractor`** (base).
- **`StaticPageExtractor`** y **`DynamicPageExtractor`** (derivadas).
- **`DataHandler`** para gestionar los datos.
- **`ScrapingCoordinator`** para coordinar el flujo del sistema.

### 3. **Entorno de Desarrollo**:

- Uso de entornos virtuales para aislamiento de dependencias (venv).
- Gesti√≥n de versiones con Git para colaboraci√≥n y control del progreso.
- Archivo **`requirements.txt`** para especificar las dependencias del proyecto.

### 4. **Salida de Datos**:

Soporte para formatos CSV, JSON o almacenamiento en bases de datos SQLite.

### 5. **Otros Requerimientos**:
Capacidad de manejar excepciones para evitar interrupciones en la ejecuci√≥n.
Compatibilidad con sitios web tanto est√°ticos como din√°micos.

# üõ†Ô∏è Configuraci√≥n del Entorno de Trabajo  

### **1. Clonar el repositorio**:
Descargar el c√≥digo fuente con los siguientes comandos:  
```bash
git clone https://github.com/santgm56/Super-Proyecto-Final.git 
cd Super-Proyecto-Final
```
### **2. Crear y activar un entorno virtual**:
El uso de un entorno virtual ayuda a instalar las dependencias del proyecto sin interferir con otras aplicaciones de Python.

**En Windows**:
```bash
python -m venv venv  
.\venv\Scripts\activate 
```

**En macOS/Linux**:
```bash
python -m venv venv  
source venv/bin/activate  
```

### **3. Instalar las dependencias**:
Una vez dentro del entorno virtual, ejecutar:

```bash
pip install -r requirements.txt 
```

### **5. Salir del entorno virtual**:
Al terminar de trabajar o hacer modificaciones, se puede salir del entorno virtual escribiendo:
```bash
deativate
```
### **Nota adicional**: 
Si se usa Windows y existe alg√∫n problema al activar el entorno virtual, es posible que se necesite habilitar la ejecuci√≥n de scripts por pol√≠ticas de resticci√≥n en powershell. Para corregirlo, basta con ejecutar estos comandos en el CMD como terminal predeterminada ya que esta no cuenta con dichas condiciones. 

# üìà Diagrama de Clases
````mermaid
classDiagram
    %% Clase base: WebDataExtractor
    class WebDataExtractor {
        - url: str
        - request_headers: dict
        + __init__(url: str, request_headers: dict)
        + get_data(): None
        + handle_exception(error: Exception): str
    }

    %% Clase derivada: StaticPageExtractor
    class StaticPageExtractor {
        + __init__(url: str, request_headers: dict)
        + get_data(): BeautifulSoup
        + extract_data(soup: BeautifulSoup, selector: str): list
    }

    %% Clase derivada: DynamicPageExtractor
    class DynamicPageExtractor {
        - driver_location: str
        + __init__(url: str, driver_location: str, request_headers: dict)
        + get_data(): BeautifulSoup
        + extract_data(soup: BeautifulSoup, selector: str): list
    }

    %% Clase para gesti√≥n de datos: DataHandler
    class DataHandler {
        - collected_data: list
        + __init__()
        + add_extracted_data(extracted_data: list): int
        + save_to_file(file_name: str): str
    }

    %% Clase controladora: ScrapingCoordinator
    class ScrapingCoordinator {
        - extractor: WebDataExtractor
        - data_handler: DataHandler
        + __init__(extractor: WebDataExtractor, data_handler: DataHandler)
        + begin_scraping(selector: str): bool
    }

    %% Relaciones
    WebDataExtractor <|-- StaticPageExtractor : herencia
    WebDataExtractor <|-- DynamicPageExtractor : herencia
    ScrapingCoordinator o-- WebDataExtractor : composici√≥n
    ScrapingCoordinator o-- DataHandler : composici√≥n
````
## **Implementaci√≥n de los Pilares de POO**
### 1. **Abstracci√≥n**:

- **Aplicaci√≥n**: La clase `WebDataExtractor` define m√©todos abstractos como `get_data()` y `handle_exception()`, lo que permite a las clases derivadas hacer sus propias implementaciones (`StaticPageExtractor` y `DynamicPageExtractor`).
- **Importancia**: se centra en lo que hace un objeto en lugar de c√≥mo lo hace, proporcionando una interfaz clara y simplificada; ideal para adaptar su implementaci√≥n en distintas situaciones.

### 2. **Encapsulamiento**:

- **Aplicaci√≥n**: Los atributos `url`, `request_headers` y `driver_location` son privados (indicados por el guion `-`), lo que significa que no pueden ser accedidos directamente desde fuera de la clase. Por lo tanto, existen m√©todos p√∫blicos (`+`) que proporcionan acceso controlado a estos atributos.
- **Importancia**: Protege los datos de acceso no autorizado y modificaciones accidentales.

### 3. **Herencia**:

- **Aplicaci√≥n**: `StaticPageExtractor` y `DynamicPageExtractor` heredan de `WebDataExtractor`, lo que significa que comparten la interfaz definida por `WebDataExtractor` y pueden proporcionar implementaciones espec√≠ficas de `get_data()` y `extract_data()`.
- **Importancia**: Promueve la reutilizaci√≥n del c√≥digo y permite crear una jerarqu√≠a de clases, lo que facilita la extensi√≥n y el mantenimiento del c√≥digo.

### 4. **Polimorfismo**:

- **Aplicaci√≥n**: La clase `ScrapingCoordinator` puede trabajar con cualquier objeto que sea una instancia de `WebDataExtractor` (ya sea `StaticPageExtractor` o `DynamicPageExtractor`), lo que permite que `begin_scraping()` funcione con diferentes tipos de extractores sin cambiar su implementaci√≥n.
- **Importancia**: Permite que una funci√≥n o un m√©todo opere sobre objetos de diferentes clases de manera uniforme sin ning√∫n problema, facilitando la extensi√≥n del c√≥digo y la integraci√≥n de nuevas clases sin modificar el c√≥digo existente.

# üíø Soluci√≥n Preliminar

### **Objetivo General**

Desarrollar un sistema de web scraping basado en la Programaci√≥n Orientada a Objetos (POO) que permita extraer, procesar y almacenar informaci√≥n de manera eficiente, utilizando Python y sus herramientas especializadas.

---
## Clases principales

### 1. **Clase WebDataExtractor**:
- `__init__(url: str, request_headers: dict = None)`:  Constructor que inicializa los atributos url (URL de la p√°gina web a scrapear) y request_headers (Encabezados HTTP para la solicitud). 
- `get_data()`: M√©todo abstracto para obtener los datos de la p√°gina web.
- `handle_exception(error: Exception)`: M√©todo para manejar excepciones.

````python
#### src/base/web_data_extractor.py

class WebDataExtractor:
    def __init__(self, url: str, request_headers: dict = None):
        self._url = url
        self._request_headers = request_headers if request_headers else {}

    def get_data(self):
        """
        M√©todo principal para obtener los datos.
        Este m√©todo debe ser implementado en las clases derivadas.
        """
        pass

    def handle_exception(self, error: Exception):
        """
        M√©todo para manejar excepciones durante el scraping.
        """
        pass
````

### 2. **Clase StaticPageExtractor**:
- `__init__(url: str, request_headers: dict = None)`: Constructor que inicializa los atributos url y request_headers llamando al constructor de la clase base.
- `get_data()`: Implementaci√≥n espec√≠fica para obtener datos de una p√°gina est√°tica.
- `extract_data(soup: BeautifulSoup, selector: str)`: M√©todo para extraer los datos obtenidos.

````python 
#### src/components/static_page_extractor.py

from src.base.web_data_extractor import WebDataExtractor
from bs4 import BeautifulSoup

class StaticPageExtractor(WebDataExtractor):
    def __init__(self, url: str, request_headers: dict = None):
        super().__init__(url, request_headers)

    def get_data(self):
         """
         M√©todo para obtener datos de una p√°gina est√°tica
         """
        pass

    def extract_data(self, soup: BeautifulSoup, selector: str):
        """
        M√©todo para extraer los datos usando BeautifulSoup
        """
        pass
````

### 3. **Clase DynamicPageExtractor**:
- `__init__(url: str, driver_location: str, request_headers: dict = None)`: Constructor que inicializa los atributos url, driver_location (Ruta del controlador del navegador para Selenium) y request_headers llamando al constructor de la clase base. 
- `get_data()`: Implementaci√≥n espec√≠fica para obtener datos de una p√°gina din√°mica.
- `extract_data(soup: BeautifulSoup, selector: str)`: M√©todo para parsear los datos obtenidos.

````python 
#### src/components/dynamic_page_extractor.py

from src.base.web_data_extractor import WebDataExtractor
from selenium import webdriver
from bs4 import BeautifulSoup

class DynamicPageExtractor(WebDataExtractor):
    def __init__(self, url: str, driver_location: str, request_headers: dict = None):
        super().__init__(url, request_headers)
        self._driver_location = driver_location

    def get_data(self):
        """
        M√©todo para obtener datos de una p√°gina din√°mica
        """
        pass

    def extract_data(self, soup: BeautifulSoup, selector: str):
        """
        M√©todo para extraer los datos usando BeautifulSoup
        """
        pass

````
### 4. **Clase DataHandler:**
- `__init__()`: Constructor que inicializa el atributo collected_data (Datos extra√≠dos) como una lista vac√≠a. 
- `add_extracted_data(new_data)`: M√©todo para a√±adir datos a la lista collected_data.
- `M√©todo para guardar los datos en un archivo.`: M√©todo para guardar los datos en un archivo.

````python 
#### src/components/data_handler.py

import pandas as pd

class DataHandler:
    def __init__(self):
        self._collected_data = []

    def add_extracted_data(self, new_data):
        """
        M√©todo para a√±adir los nuevos datos extra√≠dos al almacenamiento.
        """
        pass 

    def save_to_file(self, file_name: str):
        """
        M√©todo para guardar los datos extra√≠dos a un archivo.
        """
        pass
````

### 5. **Clase ScrapingCoordinator**:
- `__init__(extractor: WebDataExtractor, data_handler: DataHandler`: Constructor que inicializa los atributos extractor (Objeto extractor para realizar el scraping.) y data_handler (Objeto manejador de datos para gestionar los datos extra√≠dos). 
- `begin_scraping(selector: str)`: M√©todo para iniciar el proceso de scraping.

````python 
#### src/coordinator/scraping_coordinator.py

from src.base.web_data_extractor import WebDataExtractor
from src.components.data_handler import DataHandler

class ScrapingCoordinator:
    def __init__(self, extractor: WebDataExtractor, data_handler: DataHandler):
        self._extractor = extractor
        self._data_handler = data_handler

    def begin_scraping(self, selector: str):
        """
        M√©todo para inicia el proceso de scraping usando el extractor y maneja los datos extra√≠dos.
        """
        pass
````

# ‚ú® Estructura del proyecto 

```plaintext
SUPER_PROYECTO_FINAL/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ web_data_extractor.py     # Clase base WebDataExtractor
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ static_page_extractor.py  # Clase derivada StaticPageExtractor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic_page_extractor.py # Clase derivada DynamicPageExtractor
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_handler.py           # Clase DataHandler
‚îÇ   ‚îú‚îÄ‚îÄ coordinator/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraping_coordinator.py   # Clase ScrapingCoordinator
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ _init_.py
‚îÇ       ‚îî‚îÄ‚îÄ helpers.py                # Funciones auxiliares
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ _init_.py
‚îÇ   ‚îú‚îÄ‚îÄ test_static_page_extractor.py  # Pruebas unitarias de StaticPageExtractor
‚îÇ   ‚îú‚îÄ‚îÄ test_dynamic_page_extractor.py # Pruebas unitarias de DynamicPageExtractor
‚îÇ   ‚îú‚îÄ‚îÄ test_web_data_extractor.       # Pruebas unitarias de WebDataExtractor
‚îÇ   ‚îú‚îÄ‚îÄ test_data_handler.py           # Pruebas unitarias de DataHandler
‚îÇ   ‚îî‚îÄ‚îÄtest_scraping_coordinator       # Pruebas unitarias de ScrapingCoordinator
‚îú‚îÄ‚îÄ logs/
‚îÇ      error.log                      # Para errores cr√≠ticos.
‚îÇ      activiti.log                   # Para registrar eventos generales del scraping. 
‚îî‚îÄ‚îÄ outputs/
```

 A continuaci√≥n, se describe cada secci√≥n de la estructura organiza los componentes del proyecto de scraping web

### Archivos principales:
- **`README.md`**: Documento que detalla el prop√≥sito del proyecto, su configuraci√≥n, c√≥mo utilizarlo y dem√°s aspectos importantes del mismo.
- **`requirements.txt`**: Archivo que lista las dependencias necesarias para ejecutar el proyecto.
- **`setup.py`**: Archivo para empaquetar e instalar el proyecto como m√≥dulo.
- **`main.py`**: Punto de entrada principal del programa.
- **`gitinignore`**: Indica archivos y directorios que no deben ser incluidos en el repositorio.

### `src/` - C√≥digo Fuente Principal
El n√∫cleo principal del proyecto est√° dividido en m√≥dulos organizados por funcionalidad:

### `base/`
- **`web_data_extractor.py`**: Clase base `WebDataExtractor`, que contiene funcionalidades generales para la extracci√≥n de datos.

### `components/`
- **`static_page_extractor.py`**: Clase derivada `StaticPageExtractor`, dise√±ada para manejar la extracci√≥n de datos en p√°ginas est√°ticas.
- **`dynamic_page_extractor.py`**: Clase derivada `DynamicPageExtractor`, dise√±ada para la extracci√≥n de datos en p√°ginas din√°micas con contenido generado por JavaScript.
- **`data_handler.py`**: Clase `DataHandler` para procesar, limpiar y almacenar los datos extra√≠dos.

### `coordinator/`
- **`scraping_coordinator.py`**: Clase `ScrapingCoordinator` que coordina y administra el flujo del scraping en el proyecto.

### `utils/`
- **`helpers.py`**: Funciones auxiliares y herramientas de apoyo para el proyecto.

### `tests/` - Pruebas Unitarias
Contiene las pruebas para validar los componentes principales del proyecto:
- **`test_static_page_extractor.py`**: Pruebas para `StaticPageExtractor`.
- **`test_dynamic_page_extractor.py`**: Pruebas para `DynamicPageExtractor`.
- **`test_web_data_extractor.py`**: Pruebas para `WebDataExtractor`.
- **`test_data_handler.py`**: Pruebas para `DataHandler`.
- **`test_scraping_coordinator.py`**: Pruebas para el flujo coordinado de scraping.

### `logs/` - Registro de Eventos y Errores
- **`error.log`**: Archivo para registrar errores cr√≠ticos que ocurran durante la ejecuci√≥n.
- **`activity.log`**: Archivo para registrar eventos y actividades generales del scraping.

### `outputs/` - Resultados Generados
Carpeta dedicada a almacenar los datos procesados y extra√≠dos en formatos como CSV, JSON, etc.

---

# üíé **Resultados Esperados**

- Sistema funcional capaz de extraer datos de sitios web tanto est√°ticos y como din√°micos.
- Almacenamiento organizado de los datos en formatos accesibles.
- C√≥digo modular, reutilizable y escalable, que cumpla con los principios de Programcaci√≥n Orientada a Objetos (POO).
